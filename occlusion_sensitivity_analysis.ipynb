{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from org3dresnet.main import (get_inference_utils,\n",
    "                              generate_model, resume_model)\n",
    "from org3dresnet.model import generate_model, make_data_parallel\n",
    "import org3dresnet\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.backends import cudnn\n",
    "import torchvision\n",
    "from IPython.display import HTML\n",
    "from torchvision.transforms.transforms import Normalize, ToPILImage\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"/workspace/src/\")\n",
    "\n",
    "from utils.visualization import visualize\n",
    "from utils.utils import video_to_html, numericalSort\n",
    "from utils.sensitivity_analysis import OcclusionSensitivityMap3D as OSM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "opt_path = \"/workspace/data/r3d_models/finetuning/ucf101/r3d50_K_fc/opts.json\"\n",
    "with open(opt_path, \"r\") as f:\n",
    "    model_opt = json.load(f)\n",
    "model_opt = Namespace(**model_opt)\n",
    "\n",
    "model_opt.device = torch.device('cpu' if model_opt.no_cuda else 'cuda')\n",
    "if not model_opt.no_cuda:\n",
    "    cudnn.benchmark = True\n",
    "if model_opt.accimage:\n",
    "    torchvision.set_image_backend('accimage')\n",
    "\n",
    "model_opt.ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "model = generate_model(model_opt)\n",
    "model = resume_model(model_opt.resume_path, model_opt.arch, model)\n",
    "model = make_data_parallel(model, model_opt.distributed, model_opt.device)\n",
    "model.eval()\n",
    "\n",
    "model_opt.inference_batch_size = 1\n",
    "for attribute in dir(model_opt):\n",
    "    if \"path\" in str(attribute) and getattr(model_opt, str(attribute)) != None:\n",
    "        setattr(model_opt, str(attribute), Path(\n",
    "            getattr(model_opt, str(attribute))))\n",
    "inference_loader, inference_class_names = get_inference_utils(model_opt)\n",
    "\n",
    "class_labels_map = {v.lower(): k for k, v in inference_class_names.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "inputs, targets = iter(inference_loader).__next__()\n",
    "video_size = inputs[[0]].shape\n",
    "transform = inference_loader.dataset.spatial_transform\n",
    "\n",
    "_transforms = transform.transforms\n",
    "idx = [type(i) for i in _transforms].index(\n",
    "    org3dresnet.spatial_transforms.Normalize)\n",
    "normalize = _transforms[idx]\n",
    "mean = torch.tensor(normalize.mean)\n",
    "std = torch.tensor(normalize.std)\n",
    "\n",
    "unnormalize = transforms.Compose(\n",
    "    [\n",
    "        Normalize((-mean / std).tolist(), (1 / std).tolist()),\n",
    "        ToPILImage(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def load_jpg(l, g, c, n):\n",
    "    name = inference_class_names[l]\n",
    "    dir = os.path.join(\"/workspace/data/ucf101/jpg\", name,\n",
    "                       \"v_{}_g{}_c{}\".format(\n",
    "                           name, str(g).zfill(2), str(c).zfill(2))\n",
    "                       )\n",
    "    path = sorted(glob.glob(dir + \"/*\"), key=numericalSort)\n",
    "\n",
    "    target_path = path[n*16:(n+1)*16]\n",
    "    if len(target_path) < 16:\n",
    "        print(\"not exist\")\n",
    "        return False\n",
    "\n",
    "    video = []\n",
    "    for _p in target_path:\n",
    "        video.append(transform(Image.open(_p)))\n",
    "\n",
    "    return torch.stack(video)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad749ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "spatial_crop_sizes = [16]\n",
    "temporal_crop_sizes = [16]\n",
    "spatial_stride = 8\n",
    "temporal_stride = 2\n",
    "\n",
    "aosa_single = OSM(\n",
    "    net=model,\n",
    "    video_size=video_size,\n",
    "    device=model_opt.device,\n",
    "    spatial_crop_sizes=spatial_crop_sizes,\n",
    "    temporal_crop_sizes=temporal_crop_sizes,\n",
    "    spatial_stride=spatial_stride,\n",
    "    temporal_stride=temporal_stride,\n",
    "    transform=transform,\n",
    "    batchsize=400,\n",
    "    N_stack_mask=1,\n",
    ")\n",
    "\n",
    "aosa = OSM(\n",
    "    net=model,\n",
    "    video_size=video_size,\n",
    "    device=model_opt.device,\n",
    "    spatial_crop_sizes=spatial_crop_sizes,\n",
    "    temporal_crop_sizes=temporal_crop_sizes,\n",
    "    spatial_stride=spatial_stride,\n",
    "    temporal_stride=temporal_stride,\n",
    "    transform=transform,\n",
    "    batchsize=400,\n",
    "    N_stack_mask=3,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "l = 21\n",
    "g = 1  # > 0\n",
    "c = 1  # > 0\n",
    "n = 1\n",
    "\n",
    "video = load_jpg(l, g, c, n).transpose(0, 1)\n",
    "target = l\n",
    "with torch.inference_mode():\n",
    "    pred = model(video.unsqueeze(0)).cpu().numpy().argmax()\n",
    "video_orgimg = []\n",
    "\n",
    "for i in range(video_size[2]):\n",
    "    img = video.squeeze().transpose(0, 1)[i]\n",
    "    video_orgimg.append(np.array(unnormalize(img)))\n",
    "video_orgimg = np.array(video_orgimg)\n",
    "\n",
    "start = time()\n",
    "aosa_single_map = aosa_single.run(video, target)\n",
    "print(time() - start)\n",
    "\n",
    "start = time()\n",
    "aosa_map = aosa.run(video, target)\n",
    "print(time() - start)\n",
    "\n",
    "\n",
    "print(\"{0} temporal crop sizes, {1} spatial crop sizes, {2} dummy list\".format(\n",
    "    len(aosa_single_map), len(aosa_single_map[0]), len(aosa_single_map[0][0])))\n",
    "print(\"heatmap size: \", aosa_single_map[0][0][0].shape)\n",
    "\n",
    "title = \"{} (pred: {})\".format(\n",
    "    inference_class_names[l], inference_class_names[pred])\n",
    "\n",
    "\n",
    "s = visualize([aosa_single_map[0][0][0], aosa_map[0][0][0]], video_orgimg,\n",
    "    title=title,\n",
    "    )\n",
    "\n",
    "HTML(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
